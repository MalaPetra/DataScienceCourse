---
title: "Capstone Project - Milestone Report"
output: html_document
---

# Instructions

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. 

The motivation for this project is to:

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

# Review Criteria

Does the link lead to an HTML page describing the exploratory analysis of the training data set?
Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?
Has the data scientist made basic plots, such as histograms to illustrate features of the data?
Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?

###  Load data into R

```{r message=FALSE, warning=FALSE}
library(tm)
library(ggplot2)
library(wordcloud)
```

```{r}
path_us_blog <- file.path("/users/petramala/downloads/final/en_US/en_US.blogs.txt")
path_us_news <- file.path("/users/petramala/downloads/final/en_US//en_US.news.txt")
path_us_twitter <- file.path("/users/petramala/downloads/final/en_US//en_US.twitter.txt")
```

```{r}
#Read in text files
data_us_blog <- readLines(path_us_blog, encoding="UTF-8", skipNul=TRUE)
data_us_twitter <- readLines(path_us_twitter, encoding="UTF-8", skipNul=TRUE)
data_us_news <- readLines(path_us_news, encoding="UTF-8", skipNul=TRUE)
```

```{r}
head(data_us_blog,10)
head(data_us_twitter,10)
head(data_us_news,10)
```

### Information about datasets

#### Size of the files

```{r}
format(object.size(data_us_blog), units = "auto")
format(object.size(data_us_twitter), units = "auto")
format(object.size(data_us_news), units = "auto")
```

#### Number of lines

```{r}
data_us_blog_lines <- length(data_us_blog)
data_us_twitter_lines <- length(data_us_twitter)
data_us_news_lines <-length(data_us_news)

data_us_blog_lines
data_us_twitter_lines
data_us_news_lines
```

#### Maximum line length

```{r}
which.max(lapply(data_us_blog, nchar))
which.max(lapply(data_us_twitter, nchar))
which.max(lapply(data_us_news, nchar))
```
#### Number of words

```{r}
sum(sapply(strsplit(data_us_blog, "\\s+"), length))
sum(sapply(strsplit(data_us_twitter, "\\s+"), length))
sum(sapply(strsplit(data_us_news, "\\s+"), length))
```

### Pre-processing Data

#### Corpus

Join datasets and create the corpus
```{r}
# consolidating files
# getting sample of each file


sample_data_us_blog <- sample(data_us_blog, size = 1000)
sample_data_us_twitter <- sample(data_us_twitter, size = 1000)
sample_data_us_news <- sample(data_us_news, size = 1000)

JoinedData <- c(sample_data_us_blog,sample_data_us_twitter, sample_data_us_news)

MakeCorpus<- VCorpus(VectorSource(JoinedData))
```

Remove stopwords, white space, punctuation etc.

```{r}
toSpace  <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

MakeCorpus <- tm_map(MakeCorpus, toSpace,"/|@|//|$|:|:)|*|&|!|?|_|-|#|")
MakeCorpus <- tm_map(MakeCorpus, removeNumbers)
MakeCorpus <- tm_map(MakeCorpus, content_transformer(tolower))
MakeCorpus <- tm_map(MakeCorpus, removeWords, stopwords("english"))
MakeCorpus <- tm_map(MakeCorpus, removePunctuation)
MakeCorpus <- tm_map(MakeCorpus, stripWhitespace)
```

Create the DTM

```{r}
 # monogram
dtm1 <- TermDocumentMatrix(MakeCorpus)

# bigram

# library Rweka is not working on my machine due to java error. This is an alternative approach to n-grams
bigram <- function(x) {
      unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}

dtm2 <- TermDocumentMatrix(MakeCorpus, control=list(tokenize = bigram))


# trigram

trigram <- function(x) {
      unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}

dtm3 <- TermDocumentMatrix(MakeCorpus, control=list(tokenize = trigram))
```

### Data Exploration

#### Monograms Frequency

```{r}
mono_freq <- rowSums(as.matrix(dtm1))
mono_freq <- sort(mono_freq, decreasing = TRUE)
dfmono_freq <- data.frame(word = names(mono_freq), freq=mono_freq)
ggplot(dfmono_freq[1:20, ], aes(x = reorder(word, -freq), freq)) +
  geom_bar(stat="identity", fill="grey", colour="grey") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("Monogram Frequency")
```

#### 2-Gram Frequency

```{r}
bi_freq <- rowSums(as.matrix(dtm2))
bi_freq <- sort(bi_freq, decreasing = TRUE)
dfbi_freq <- data.frame(word = names(bi_freq), freq=bi_freq)
ggplot(dfbi_freq[1:20, ], aes(x = reorder(word, -freq), freq)) +
  geom_bar(stat="identity", fill="yellow", colour="grey") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("Bigram Frequency")
```

#### 3-Gram Frequency

```{r}
tri_freq <- rowSums(as.matrix(dtm3))
tri_freq <- sort(tri_freq, decreasing = TRUE)
dftri_freq <- data.frame(word = names(tri_freq), freq=tri_freq)
ggplot(dftri_freq[1:20, ], aes(x = reorder(word, -freq), freq)) +
  geom_bar(stat="identity", fill="purple", colour="grey") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("Trigram Frequency")
```

#### WordCloud

```{r}
# Vector of terms
terms_vec <- names(mono_freq)

wordcloud(terms_vec, mono_freq, max.words = 50, colors = "blue")
```

### Interesting findings

I came across a java issue with loading Rweka library which I was't able to fix in a reasonable time. Instead I found alternative solution without using this library which you can find in Create the DTM part.

It's also useful to see how similarities in text decrease with n-grams. 

If I would want to explore this even further, it would be useful to look at it not just for the joined subset but each file separately to see if there isn't particular issue in one them.
