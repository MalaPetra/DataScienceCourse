---
title: "Capstone Project Week 3 + Quiz Week 2"
output: html_document
---


# Data Cleanse (English Twitter texts) and ngram

1. Convert all letters to lower case to simplify the problem;
2. Split lines at “.”, “,” and etc.;
3. Remove non-alphanumeric characters at the beginning or at the end of a word, retain special characters in words like “i’am”, “we’ve”, etc;
4. Remove extra spaces;
5. Split words by space.

```{r}
library(stringr)
library(tm)
```

```{r}
# read file
path_us_twitter <- file.path("/users/petramala/downloads/final/en_US//en_US.twitter.txt")

filepath <- "/users/petramala/downloads/final/en_US//en_US.twitter.txt"
con <- file(filepath) 
lines <- readLines(con, skipNul=TRUE) # 2360148 lines
close(con)

lines <- tolower(lines)
# split at all ".", "," and etc.
lines <- unlist(strsplit(lines, "[.,:;!?(){}<>]|[][]+"))

# replace all non-alphanumeric characters with a space at the beginning/end of a word.
  lines <- gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", lines) # at the begining/end of a line
  lines <- gsub("[^a-z0-9]+\\s", " ", lines) # before space
  lines <- gsub("\\s[^a-z0-9]+", " ", lines) # after space
  # split a string at spaces then remove the words 
  # that contain any non-alphabetic characters (excpet "-", "'")
  # then paste them together (separate them with spaces)
  lines <- unlist(lapply(lines, function(line){
    words <- unlist(strsplit(line, "\\s+"))
    words <- words[!grepl("[^a-z'-]", words, perl=TRUE)]
    paste(words, collapse=" ")}))
  
  # remove axcess spaces
  #lines <- gsub("\\s+", " ", lines) # remove mutiple spaces
  lines <- str_trim(lines) # remove spaces at the beginning/end of the line

  # drop blank lines
  lines <- lines[nchar(lines)>0]
saveRDS(lines, file="co_tidy_twitter_en.rds")
```

Check how the texts look like now.

```{r}
lines <- readRDS(file="co_tidy_twitter_en.rds")
head(lines, 20)
```

Remove the stop words.

```{r}
# remove stop words (There are 5398319 lines, size 356.3 Mb. With 16 Gb memory it took about 3 hours. According to this post https://stackoverflow.com/questions/50635341/removing-stop-words-from-corpus-in-r-is-too-slow , it took 2 hours to process 31 MB data by using package "tm".)

#lines <- unlist(lapply(lines, function(line){removeWords(line, stopwords("en"))}))
#lines <- str_trim(lines) # remove spaces at the beginning/end of the line
#lines <- gsub("\\s+", " ", lines) # remove mutiple spaces
#lines <- lines[nchar(lines)>0] # remove blank lines. reduce the elements from 5398319 to 5059787
#saveRDS(lines, file=co_tidy_nostop_twitter_en)
```

# Count word frequence (1-gram)

```{r}
# split words by space
words <- unlist(strsplit(lines, "\\s+"))

# count word frequence
word.freq <- table(words)

# convert to data frame
df <- cbind.data.frame(names(word.freq), as.integer(word.freq))
names(df) <- c('word', 'freq')
row.names(df) <- df[,1]

# sort words by frequence descending
df <- df[order(-df$freq),]

# save as RDS file
saveRDS(df, file="co_1gram_twitter_en")
```

Locate a word in the list.

```{r}
# read word frequence data
df <- readRDS(file="co_1gram_twitter_en")
# locate "i'm" 
df["i'm",]
```

# Plotting

```{r}
ggplot(df[1:20,], aes(x=reorder(word,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Twitter Top 20 Word Frequence (with Stop Words)", x=NULL, y="Word Frequency")
```

```{r}
plot(df[1:500,]$freq,
     main='Twitter Top 500 Word Frequence',
     ylab="Frequence",
     xlab="Word")
```

```{r}
ggplot(data=df[1:250,], aes(x=df[1:250,]$freq)) + 
  geom_histogram(colour="black", fill="white", breaks=seq(0, 900000,by=3000)) + 
  labs(title="Histogram of Twitter Top 250 Word Frequence", x="Word Frequency", y="Count")
```


Instead, stop words are removed from the frequence data frame directly.

```{r}
df <- readRDS(file="co_1gram_twitter_en")
head(df)
lines <- lapply(lines, function(line){removeWords(line, stopwords("en"))})


saveRDS(df, "co_1gram_nostop_twitter_en")

df <- readRDS("co_1gram_nostop_twitter_en")

ggplot(df[1:20,], aes(x=reorder(word,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Twitter Top 20 Word Frequence (without Stop Words)", x=NULL, y="Word Frequency")
```

```{r}
library("RColorBrewer")
library("wordcloud")
# generate word cloud
set.seed(1234)
wordcloud(words = df$word, freq = df$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

# 2-grams

For example, generate 2-grams from sentence “how are you”. Refer to document: https://cran.r-project.org/web/packages/ngram/vignettes/ngram-guide.pdf

```{r}
lines[2]
```

```{r}
library(ngram)
print(ngram(lines[2], n=2), output="full")
```

Now generate 2-grams from the whole Twitter text.

```{r}
# remove lines that contain less than 2 words, or ngram() would throw errors.
lines <- lines[str_count(lines, "\\s+")>0] # 4375507 lines
bigram <- ngram(lines, n=2) # this line takes long time. probably should sample the text first.
df <- get.phrasetable(bigram)
saveRDS(df, co_2gram_twitter_en)
df <- readRDS(co_2gram_twitter_en)
ggplot(df[1:20,], aes(x=reorder(ngrams,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Twitter Top 20 2-Gram Frequence (with Stop Words)", x=NULL, y="Word Frequency")
```

# 3-grams

Generate 3-grams for the whole Twitter text.

```{r}
lines <- readRDS(co_tidy_twitter_en)
# remove lines that contain less than 3 words, or ngram() would throw errors.
lines <- lines[str_count(lines, "\\s+")>1] # 3803575 lines
trigram <- ngram(lines, n=3) # this doesn't take long time surprisingly.
df <- get.phrasetable(trigram)
saveRDS(df, co_3gram_twitter_en)
df <- readRDS(co_3gram_twitter_en)
ggplot(df[1:20,], aes(x=reorder(ngrams,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Twitter Top 20 3-Gram Frequence (with Stop Words)", x=NULL, y="Word Frequency")
```

```{r}
lines <- readRDS((co_tidy_nostop_twitter_en))
# remove lines that contain less than 3 words, or ngram() would throw errors.
lines <- lines[str_count(lines, "\\s+")>2] # 2780871 lines left
trigram <- ngram(lines, n=3) # this took less than a minute surprisingly.
df <- get.phrasetable(trigram)
saveRDS(df, co_3gram_nostop_twitter_en)
df <- readRDS(co_3gram_nostop_twitter_en)
ggplot(df[1:20,], aes(x=reorder(ngrams,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Twitter Top 20 3-Gram Frequence (without Stop Words)", x=NULL, y="Word Frequency")
```

# Unique words needed to cover the text

```{r}
df <- readRDS(co_1gram_twitter_en) # with stop words
df$count <- 1
df$count <- cumsum(df$count)
df$coverage <- cumsum(df$freq) / sum(df$freq) * 100
df <- df[df$coverage <= 91,]

# find the word counts for 50% and 90% coverage 
points <- rbind(tail(df[df$coverage <= 50,], 1), tail(df[df$coverage <= 90,], 1))

ggplot(data=df, aes(x=count, y=coverage, group=1)) +
  geom_line()+
  geom_point(data=points, colour="red", size=3) +
  geom_text(data=points, aes(label=count), hjust=-1, vjust=1) +
  ggtitle("Number of Words to Cover Twitter Text (with Stop Words)") +
  xlab("Number of Words") +
  ylab("Coverage Percentage")
```

Removal of the stop words would increase the number of words needed to cover the whole text.To decrease the number, lemmatization and/or stemming could be used. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/

For other two English files
Do the same to see whether there are differences.

```{r}
df_news <- readRDS("D:/R/capstone/data/1gram_nostop_news_en.rds")
df_blogs <- readRDS("D:/R/capstone/data/1gram_nostop_blogs_en.rds")

par(mfrow=c(1,2))

df <- readRDS(co_1gram_nostop_news_en)
ggplot(df_news[1:20,], aes(x=reorder(word,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="News Top 20 Word Frequence (without Stop Words)", x=NULL, y="Word Frequency")

df <- readRDS(co_1gram_nostop_blogs_en)
ggplot(df_blogs[1:20,], aes(x=reorder(word,freq), freq)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Blogs Top 20 Word Frequence (without Stop Words)", x=NULL, y="Word Frequency")
```

# Quiz

### Quiz 2 Data 

```{r}
library(stringr)
library(ngram)

# read file
filepath <- "/users/petramala/downloads/final/en_US//en_US.twitter.txt"
con <- file(filepath) 
lines <- readLines(con, skipNul=TRUE) # 2360148 lines
close(con)

lines <- tolower(lines)
# split at all ".", "," and etc.
lines <- unlist(strsplit(lines, "[.,:;!?(){}<>]+")) # 5398319 lines

# replace all non-alphanumeric characters with a space at the beginning/end of a word.
lines <- gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", lines) # at the begining/end of a line
lines <- gsub("[^a-z0-9]+\\s", " ", lines) # before space
lines <- gsub("\\s[^a-z0-9]+", " ", lines) # after space
lines <- gsub("\\s+", " ", lines) # remove mutiple spaces
lines <- str_trim(lines) # remove spaces at the beginning/end of the line
saveRDS(lines, file="co_tidy_twitter_en")




filepath <- "/users/petramala/downloads/final/en_US//en_US.news.txt"
con <- file(filepath) 
lines <- readLines(con, skipNul=TRUE) # 2360148 lines
close(con)

lines <- tolower(lines)
# split at all ".", "," and etc.
lines <- unlist(strsplit(lines, "[.,:;!?(){}<>]+")) # 5398319 lines

# replace all non-alphanumeric characters with a space at the beginning/end of a word.
lines <- gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", lines) # at the begining/end of a line
lines <- gsub("[^a-z0-9]+\\s", " ", lines) # before space
lines <- gsub("\\s[^a-z0-9]+", " ", lines) # after space
lines <- gsub("\\s+", " ", lines) # remove mutiple spaces
lines <- str_trim(lines) # remove spaces at the beginning/end of the line
saveRDS(lines, file="co_tidy_news_en")



filepath <-"/users/petramala/downloads/final/en_US/en_US.blogs.txt"
con <- file(filepath) 
lines <- readLines(con, skipNul=TRUE) # 2360148 lines
close(con)

lines <- tolower(lines)
# split at all ".", "," and etc.
lines <- unlist(strsplit(lines, "[.,:;!?(){}<>]+")) # 5398319 lines

# replace all non-alphanumeric characters with a space at the beginning/end of a word.
lines <- gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", lines) # at the begining/end of a line
lines <- gsub("[^a-z0-9]+\\s", " ", lines) # before space
lines <- gsub("\\s[^a-z0-9]+", " ", lines) # after space
lines <- gsub("\\s+", " ", lines) # remove mutiple spaces
lines <- str_trim(lines) # remove spaces at the beginning/end of the line
saveRDS(lines, file="co_tidy_blogs_en")

```

```{r}
df_news <- readRDS("co_tidy_news_en")
df_blogs <- readRDS("co_tidy_blogs_en")
df_twitter <- readRDS("co_tidy_twitter_en")
```

```{r}
lines <- c(df_news, df_blogs, df_twitter)
```

```{r}
# remove lines that contain less than 3 words, or ngram() would throw errors.
lines_removed <- lines[str_count(lines, "\\s+")>1] # 3803575 lines
#rm(df_news, df_blogs, df_twitter)

trigram <- ngram(lines_removed, n=3) # this doesn't take long time surprisingly.
#rm(lines)

final <- get.phrasetable(trigram)
saveRDS(final, "co_3gram_twitter_en")

df <-  readRDS("co_3gram_twitter_en")
head(df,100)
```

### Quiz Questions and Solution

**Question 1**

For each of the sentence fragments below use your natural language processing algorithm to predict the next word in the sentence.

The guy in front of me just bought a pound of bacon, a bouquet, and a case of

```{r}
head(df[grep("^case of", df[,1]),], 10)
```

**Question 2**

You're the reason why I smile everyday. Can you follow me please? It would mean the

```{r}
head(df[grep("^mean the", df[,1]),], 10)
```

**Question 3**

Hey sunshine, can you follow me and make me the

```{r}
rbind(df[grep("^me the bluest", df[,1]),],
      df[grep("^me the smelliest", df[,1]),],
      df[grep("^me the saddest", df[,1]),],
      df[grep("^me the happiest", df[,1]),])
```

**Question 4**

Very early observations on the Bills game: Offense still struggling but the

```{r}
head(df[grep("^struggling but", df[,1]),], 30)

# There are only 3 options and none of them is in the answer on coursera
```


**Question 5**

Go on a romantic date at the

```{r}
rbind(df[grep("^date at mall", df[,1]),],
      df[grep("^date at grocery", df[,1]),],
      df[grep("^date at movie", df[,1]),],
      df[grep("^date at beach", df[,1]),])
```

```{r}
head(df[grep("^date at", df[,1]),],100)
# none of the option is in en file
```

**Question 6**

Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my

```{r}
head(df[grep("^on my ", df[,1]),], 10)
```

**Question 7**

Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some

```{r}
head(df[grep("^quite some ", df[,1]),], 10)
```

**Question 8**

After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little

```{r}
head(df[grep("^his little", df[,1]),], 20)
```


```{r}
rbind(df[grep("his little fingers", df[,1]),], 
      df[grep("his little eye", df[,1]),],
      df[grep("his little ear", df[,1]),], 
      df[grep("his little toe", df[,1]),])
```

**Question 9**

Be grateful for the good times and keep the faith during the


```{r}
head(df[grep("^during the ", df[,1]),], 20)
```

```{r}
rbind(df[grep("during the sad", df[,1]),], 
      df[grep("during the worse", df[,1]),],
      df[grep("during the bad", df[,1]),], 
      df[grep("during the hard", df[,1]),])
```

**Question 10**

```{r}
head(df[grep("^must be ", df[,1]),], 10)
```

```{r}
rbind(df[grep("must be asleep", df[,1]),], 
      df[grep("must be insensitive", df[,1]),],
      df[grep("must be insane", df[,1]),], 
      df[grep("must be callous", df[,1]),])
```

