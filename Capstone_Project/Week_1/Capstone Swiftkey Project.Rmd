---
title: "Data Science Capstone Project"
output: html_document
---

# Project Requirements

The capstone will be evaluated based on the following assessments:

- An introductory quiz to test whether you have downloaded and can manipulate the data.
- An intermediate R markdown report that describes in plain language, plots, and code your exploratory analysis of the course data set.
- Two natural language processing quizzes, where you apply your predictive model to real data to check how it is working.
- A Shiny app that takes as input a phrase (multiple words), one clicks submit, and it predicts the next word.
- A 5 slide deck created with R presentations pitching your algorithm and app to your boss or investor.


# Course Tasks

This course will be separated into 8 different tasks that cover the range of activities encountered by a practicing data scientist. They mirror many of the skills you have developed in the data science specialization. The tasks are:

- Understanding the problem
- Data acquisition and cleaning
- Exploratory analysis
- Statistical modeling
- Predictive modeling
- Creative exploration
- Creating a data product
- Creating a short slide deck pitching your product


Data: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

Project Focus: NLP, text mining

# About the Corpora

The corpora are collected from publicly available sources by a web crawler. The crawler checks for language, so as to mainly get texts consisting of the desired language*.

Each entry is tagged with it's date of publication. Where user comments are included they will be tagged with the date of the main entry.

Each entry is tagged with the type of entry, based on the type of website it is collected from (e.g. newspaper or personal blog) If possible, each entry is tagged with one or more subjects based on the title or keywords of the entry (e.g. if the entry comes from the sports section of a newspaper it will be tagged with "sports" subject).In many cases it's not feasible to tag the entries (for example, it's not really practical to tag each individual Twitter entry, though I've got some ideas which might be implemented in the future) or no subject is found by the automated process, in which case the entry is tagged with a '0'.

To save space, the subject and type is given as a numerical code.

Once the raw corpus has been collected, it is parsed further, to remove duplicate entries and split into individual lines. Approximately 50% of each entry is then deleted. Since you cannot fully recreate any entries, the entries are anonymised and this is a non-profit venture I believe that it would fall under Fair Use.

### Corpus Sample

tagesspiegel.de 2010/12/03 1 7 Er ist weder ein Abzocker noch ein Ausbeuter, er ist kein Betrger, er haut niemanden in die Pfanne oder betrgt ihn um seinen gerechten Anteil, er steht zu seinem Wort und erfllt seine Vertrge sinngem und feilscht nicht wegen irgendwelcher Lcken im Maschendraht des Kleingedruckten der Vertrge.spiegel.de 2010/11/30 1 1,6 Diplomaten sehen Clintons Direktive als Besttigung einer alten Regel: Diezeit.de 2009/10/22 1 2,10 Warum schaffen wir nicht eine Whrung, die diese Aufgaben erfllt anstatt den Forderungen der Geldwirtschaft hinterherzuhecheln, die niemals erfllt werden knnen?

* You may still find lines of entirely different languages in the corpus. There are 2 main reasons for that:1. Similar languages. Some languages are very similar, and the automatic language checker could therefore erroneously accept the foreign language text.2. "Embedded" foreign languages. While a text may be mainly in the desired language there may be parts of it in another language. Since the text is then split up into individual lines, it is possible to see entire lines written in a foreign language.Whereas number 1 is just an out-and-out error, I think number 2 is actually desirable, as it will give a picture of when foreign language is used within the main language.

# Getting and Cleaning the Data

The two basic goals for this task are tokenization and profanity filtering. So the basic idea is that you want to be able to take a bunch of text and divide it into what we would call words. There are a number of issues that you're going to to have to deal with here, including things like how to handle punctuation, how to think about digits of capital and lowercase letters and how to deal with typos, because people can spell things incorrectly. So, as you're coding your solution, you're going to have to think of an optimal strategy for dealing with all of these issues you know, and think about them in a way that optimizes performance but also accuracy


**Tokenization** - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

**Profanity filtering** - removing profanity and other words you do not want to predict.

### Tips, tricks, and hints

This dataset is fairly large. We emphasize that you don't necessarily need to load the entire dataset in to build your algorithms (see point 2 below). At least initially, you might want to use a smaller subset of the data. Reading in chunks or lines using R's readLines or scan functions can be useful. You can also loop over each line of text by embedding readLines within a for/while loop, but this may be slower than reading in large chunks at a time. Reading pieces of the file at a time will require the use of a file connection in R. 

For example, the following code could be used to read the first few lines of the English Twitter dataset:

```
#con <- file("en_US.twitter.txt", "r") 
#readLines(con, 1) 
## Read the first line of text readLines(con, 1) 
## Read the next line of text readLines(con, 5)
## Read in the next 5 lines of text close(con) 
## It's important to close the connection when you are done. See the connections help page for more information.
```

Sampling. 

To reiterate, to build models you don't need to load in and use all of the data. Often relatively few randomly selected rows or chunks need to be included to get an accurate approximation to results that would be obtained using all the data. Remember your inference class and how a representative sample can be used to infer facts about a population. You might want to create a separate sub-sample dataset by reading in a random subset of the original data and writing it out to a separate file. That way, you can store the sample and not have to recreate it every time. You can use the rbinom function to "flip a biased coin" to determine whether you sample a line of text or not.

### Quiz 1

```{r}
# load the data
#url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
#download.file(url, destfile = "Coursera-SwiftKey.zip")
#unzip("Coursera-SwiftKey.zip")

#Setting file paths to access the file directly

path_us_blog <- file.path("/users/petramala/downloads/final/en_US/en_US.blogs.txt")
path_us_news <- file.path("/users/petramala/downloads/final/en_US//en_US.news.txt")
path_us_twitter <- file.path("/users/petramala/downloads/final/en_US//en_US.twitter.txt")

#Read in text files

data_us_blog <- readLines(path_us_blog, encoding="UTF-8", skipNul=TRUE)

data_us_twitter <- readLines(path_us_twitter, encoding="UTF-8", skipNul=TRUE)

data_us_news <- readLines(path_us_news, encoding="UTF-8", skipNul=TRUE)
```

#### Question 1

The 𝚎𝚗_𝚄𝚂.𝚋𝚕𝚘𝚐𝚜.𝚝𝚡𝚝 file is how man

Answer: 200.4MB

#### Question 2 

The 𝚎𝚗_𝚄𝚂.𝚝𝚠𝚒𝚝𝚝𝚎𝚛.𝚝𝚡𝚝 has how many lines of text?

```{r}
length(data_us_twitter)
```

#### Question 3

What is the length of the longest line seen in any of the three en_US data sets?

```{r}
max(nchar(data_us_blog))
max(nchar(data_us_twitter))
max(nchar(data_us_news))
```

Answer: 40833 in the blogs data set

#### Question 4

In the en_US twitter data set, if you divide the number of lines where the word “love” (all lowercase) occurs by the number of lines the word “hate” (all lowercase) occurs, about what do you get?

```{r}
# https://bookdown.org/rdpeng/rprogdatascience/regular-expressions.html#grep

twitter_love <- grep("love", data_us_twitter)
twitter_hate <- grep("hate", data_us_twitter)
length(twitter_love)/length(twitter_hate)
```

Answer: 4.108592

#### Question 5

The one tweet in the en_US twitter data set that matches the word “biostats” says what?

```{r}
data_us_twitter[grep("biostats", data_us_twitter)]
```


Answer:  i know how you feel.. i have biostats on tuesday and i have yet to study =/

#### Question 6

How many tweets have the exact characters “A computer once beat me at chess, but it was no match for me at kickboxing”. (I.e. the line matches those characters exactly.)

```{r}
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing", data_us_twitter))
```

Answer: 3
